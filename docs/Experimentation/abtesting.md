---
title: 2 - A/B Testing
layout: page
---

# ğŸ§ª A/B Testing

## ğŸ” Definition
A randomized experiment comparing two versions:

- **A (Control):** current baseline.
- **B (Treatment):** new variant.

> ğŸ¨ *Optional hero image (place in `/assets/ab-testing/`):*
>
> <figure>
>   <img src="/assets/ab-testing/ab-hero.png" alt="Illustration of traffic split into A (control) and B (treatment) with metric comparison panels" />
>   <figcaption>High-level view of a randomized split and metric comparison.</figcaption>
> </figure>
>
> <details>
> <summary>ğŸ§‘â€ğŸ’» Suggested AI prompt</summary>
> Minimal flat illustration of an A/B test: traffic enters a split node and flows into two panels labeled **A (Control)** and **B (Treatment)**, each showing a conversion gauge. Clean, modern, vector style, white background, subtle shadows, brand-neutral colors.
> </details>

---

## ğŸ§  Statistical framework
- **Null hypothesis (Hâ‚€):** no difference between A and B.
- **Alternative (Hâ‚):** a difference exists (two-sided by default; use one-sided only with strong prior and pre-registration) âš ï¸.
- **Why A/B tests?**  
  - âœ… Simple, robust design  
  - ğŸ² Randomization minimizes selection bias  
  - ğŸ“ Quantifies uncertainty for clear decisions

---

## ğŸ§­ Minimal workflow
1. ğŸ¯ Define the **primary metric** (e.g., conversion rate, revenue per user).
2. ğŸ‘¤ Choose **unit of randomization** (user, session, PNR, household).
3. ğŸš¦ Set **exposure rules** (who can/cannot be re-exposed).
4. ğŸ“ Pre-register **Î±, power, MDE, duration, guardrails**.
5. ğŸ“Š Run, monitor for **sample ratio mismatch (SRM)**.
6. ğŸ§¾ Analyze and **decide** (see decision rules below).

---

## ğŸ§© Design choices that matter

### ğŸ†” Unit of randomization
- Prefer **stable identifiers** (user ID, PNR) over sessions/cookies when possible.
- If cross-device is common, consider **household** or **account** level to reduce spillovers.

### âš–ï¸ Split & allocation
- **50/50** maximizes power; skewed splits (e.g., 90/10) need **larger total N** for the same MDE.
- For risky changes, start with a **10â€“20% ramp**, then escalate if guardrails are healthy.

### ğŸ§± Stratification (blocking)
- Randomize **within strata** (e.g., country, device, recency tiers) to keep groups balanced on key covariates.
- Report results overall and by strata; pre-specify subgroup analyses.

### ğŸ”’ Exposure policy
- One user â†’ one arm for the full test window.
- Avoid cross-arm exposure (e.g., receiving both Email and WhatsApp).
- Define **frequency caps** and **eligibility** rules up front.

---

## ğŸ“ Metrics & statistical tests

### âœ… Binary metrics (conversion rate, click-through rate)
Let \( p_A \), \( p_B \) be conversion rates; \( n_A, n_B \) sample sizes.

- **Difference:** \( \Delta = p_B - p_A \)
- **Pooled SE (for Hâ‚€: \( p_A = p_B \)):**
  \[
  \hat{p} = \frac{x_A + x_B}{n_A + n_B}, \quad
  SE_{\text{pooled}} = \sqrt{\hat{p}(1-\hat{p})\left(\frac{1}{n_A}+\frac{1}{n_B}\right)}
  \]
- **z-statistic:** \( z = \frac{\Delta}{SE_{\text{pooled}}} \)
- **Two-sided p-value:** compare |z| to standard normal.

> **95% CI (unpooled)**  
> \[
> CI = \Delta \pm 1.96 \cdot \sqrt{\frac{p_A(1-p_A)}{n_A} + \frac{p_B(1-p_B)}{n_B}}
> \]

### ğŸ’µ Continuous metrics (Revenue per User, Order Value)
- Use **Welchâ€™s t-test** (unequal variances).
- For skewed revenue: **trimmed means**, **winsorization**, or **bootstrap CIs**.
- Report both **mean** and **median**; consider **RPU** and **RPT**.

### â±ï¸ Ratio & time-based metrics
- If numerator/denominator vary per user, consider **delta method** or **bootstrap**.
- Guardrails (refund rate, complaint rate) are tested but **not optimized**.

---

## ğŸ“ Sample size, Power, and MDE

- **Type I error (Î±):** false positive rate (commonly 0.05).
- **Type II error (Î²):** false negative rate; **Power = 1 âˆ’ Î²** (commonly 0.8).

**Back-of-the-envelope N for two proportions** *(per arm)*:
\[
n \approx \frac{2\,p(1-p)\,(z_{1-\alpha/2}+z_{1-\beta})^2}{\delta^2}
\]
Where \( p \) is baseline rate; \( \delta \) is **absolute** lift (MDE, in pp).

> ğŸ§® **Interpretation:** Smaller MDE â‡’ larger sample or longer duration.

---

## ğŸ¯ Variance reduction (optional but powerful)

- **Stratified randomization:** balances covariates â‡’ lower variance.
- **Covariate adjustment (ANCOVA):** regress outcome on treatment + pre-exposure covariates.
- **CUPED (pre-exposure correction):**  
  \( y' = y - \theta (x - \bar{x}) \), with \( \theta = \frac{\text{Cov}(y,x)}{\text{Var}(x)} \).


âš ï¸ Pre-specify the method; ensure treatment does **not** affect the covariate \( x \).

---

## ğŸ©º Randomization health & SRM

**Sample Ratio Mismatch (SRM):** observed allocation deviates from expected (e.g., expect 50/50, observe 49/51 out of 200k).

- Diagnose with a **chi-square test** on assignment counts.
- Common causes: bot traffic, bucketing bugs, eligibility filters applied **after** randomization, cross-device leakage, late-day traffic spikes.


 <details>
 <summary>ğŸ§‘â€ğŸ’» Suggested AI prompt</summary>
 Clean bar chart comparing expected 50/50 vs observed 49/51 assignment with a red highlight on the mismatch. Minimalist dashboard style.
 </details>

**Action:** If SRM is significant, **invalidate** the test and fix instrumentation before re-running.

---

## â³ Stopping rules & sequential looks

- **Fixed-horizon testing:** set duration and N in advance; **no peeking**.
- **Sequential testing:** allowed with proper **alpha-spending** (e.g., Pocock, Oâ€™Brienâ€“Fleming) or **always-valid** methods.
- If monitoring periodically, use **group-sequential** plans and document spending rules.

---

## â— Multiple comparisons

If you test many variants/metrics/subgroups, control false discoveries:

- **Bonferroni** (conservative), **Holmâ€“Bonferroni**, or **Benjaminiâ€“Hochberg (FDR)**.
- Prefer a **single primary metric**; treat others as **guardrails** or **exploratory**.

---

## ğŸ› ï¸ Worked examples

### ğŸ“ˆ Example 1 â€” Conversion rate lift
- \( n_A = n_B = 50{,}000 \)
- \( p_A = 3.0\% = 0.030 \)
- \( p_B = 3.6\% = 0.036 \)
- \( \Delta = 0.006 \) (**+0.6 pp**)

Pooled \( \hat{p} = 0.03299 \)  
\( SE_{\text{pooled}} \approx 0.00113 \)  
\( z \approx 5.31 \) â†’ **p-value â‰ˆ 1.1eâˆ’7** âœ…  
95% CI (unpooled) for \( \Delta \): **[+0.38 pp, +0.82 pp]**  
**Decision:** reject Hâ‚€. Also check **business impact** (incremental revenue, margin).

### ğŸ’° Example 2 â€” Revenue per user (skewed)
- Use **Welchâ€™s t-test** or **bootstrap CI** on mean RPU.  
- If heavy tails: **winsorize** top 0.1â€“1% or report **trimmed mean (e.g., 5%)**.  
- Complement with **conversion rate** and **AOV** to explain drivers.

---

## ğŸ§¯ Common pitfalls & remedies

- ğŸ‘€ **Peeking without correction:** inflates Type I error â†’ use fixed horizon or sequential designs.  
- ğŸ” **Inconsistent exposure:** users switch arms â†’ lock assignment, deduplicate IDs.  
- ğŸŒ **Interference/spillovers:** cross-channel effects â†’ consider **geo experiments** or **cluster randomization**.  
- ğŸ“… **Seasonality & novelty:** ensure runtime covers weekly cycles; consider ramps and post-holdouts.  
- ğŸ§® **Metric drift:** freeze definitions during the test.  
- ğŸ§ª **Underpowered tests:** align MDE with business value; extend duration or traffic.  

---

## âœ… Decision checklist
- [ ] Primary metric defined; guardrails listed.  
- [ ] Î±, power, MDE pre-registered; sample size computed.  
- [ ] Unit of randomization & exposure rules documented.  
- [ ] Stratification keys (if any) fixed pre-launch.  
- [ ] SRM monitored; assignment counts logged.  
- [ ] Fixed horizon **or** sequential plan documented.  
- [ ] Analysis plan: test type, CI, variance reduction (if any).  
- [ ] Result meets **statistical** and **business** significance.  
- [ ] Rollout plan & post-launch monitoring defined.  

---

