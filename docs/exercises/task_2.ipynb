{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "014de34a",
   "metadata": {},
   "source": [
    "\n",
    "# Cross Validation with Linear Regression — Exercises\n",
    "\n",
    "> **Dataset:** We'll use `load_diabetes()` from scikit-learn (regression, 10 numeric features).  \n",
    "> **Metrics:** Use **RMSE** (root mean squared error).  \n",
    "> **CV Scheme:** Use **KFold** (e.g., 5 folds, shuffling with a fixed random state for reproducibility).\n",
    "\n",
    "Run the **Setup** cell once, then work through Exercises 1–3. Fill in the `# TODO` parts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Setup (Run once) =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn uses negative RMSE for scoring to keep \"higher is better\"\n",
    "RMSE_SCORING = \"neg_root_mean_squared_error\"\n",
    "\n",
    "# Load data\n",
    "data = load_diabetes()\n",
    "X = data.data          # shape (442, 10)\n",
    "y = data.target        # quantitative target\n",
    "\n",
    "# Optional: peek at the data\n",
    "pd.DataFrame(X, columns=data.feature_names).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a42b3",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 1 — Train/Test Split vs. K-Fold Cross-Validation\n",
    "\n",
    "**Goal:** Compare a single 80/20 train–test split with a **5-fold cross-validation** estimate using **Linear Regression**. Interpret why CV is typically a better performance estimate.\n",
    "\n",
    "**What to learn**\n",
    "- Why a single split can be noisy.\n",
    "- How K-fold reduces variance in the estimate.\n",
    "\n",
    "**Your task**\n",
    "1. Do an **80/20 split**, fit `LinearRegression`, compute **test RMSE**.  \n",
    "2. Do **5-fold CV** on the whole dataset and compute **mean RMSE** across folds.  \n",
    "3. Compare and briefly explain the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e128e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Part A: 80/20 split RMSE ---\n",
    "\n",
    "# TODO: create X_train, X_test, y_train, y_test with test_size=0.2 and random_state=42\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# TODO: build a Pipeline with StandardScaler() and LinearRegression()\n",
    "# pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n",
    "\n",
    "# TODO: fit the pipeline on the training data\n",
    "# pipe.fit(...)\n",
    "\n",
    "# TODO: predict on test, compute RMSE\n",
    "# y_pred = pipe.predict(...)\n",
    "# rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "# print(f\"Test RMSE (single 80/20 split): {rmse:.2f}\")\n",
    "\n",
    "\n",
    "# --- Part B: 5-fold CV RMSE ---\n",
    "\n",
    "# TODO: define KFold with n_splits=5, shuffle=True, random_state=42\n",
    "# kf = KFold(...)\n",
    "\n",
    "# TODO: use cross_val_score with the same pipeline, scoring=RMSE_SCORING and cv=kf\n",
    "# scores = cross_val_score(pipe, X, y, scoring=RMSE_SCORING, cv=kf)\n",
    "\n",
    "# Convert to positive RMSE\n",
    "# rmses = -scores\n",
    "# mean_rmse = rmses.mean()\n",
    "# std_rmse = rmses.std()\n",
    "# print(f\"CV RMSE (5-fold): {mean_rmse:.2f} ± {std_rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca803e7",
   "metadata": {},
   "source": [
    "\n",
    "**Reflection (write here):**  \n",
    "Explain in 2–4 sentences why the CV estimate can be more reliable than a single split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b9e86d",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2 — How Many Features Help? (Feature Count vs. CV Error)\n",
    "\n",
    "**Goal:** Examine how performance changes as you vary the **number of input features** (e.g., first 2, 4, 6, 8, 10 features).\n",
    "\n",
    "**What to learn**\n",
    "- Adding features can reduce bias but may increase variance/noise.\n",
    "- CV is a fair way to compare feature sets.\n",
    "\n",
    "**Your task**\n",
    "1. For `k` in `{2, 4, 6, 8, 10}`, select `X[:, :k]`.  \n",
    "2. Use a **Pipeline(StandardScaler → LinearRegression)**.  \n",
    "3. Compute **5-fold CV RMSE** for each `k`.  \n",
    "4. Plot `k` vs **mean RMSE**.  \n",
    "5. Briefly discuss the trend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2affebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Feature Count vs. CV Error ---\n",
    "\n",
    "k_values = [2, 4, 6, 8, 10]\n",
    "mean_rmses, std_rmses = [], []\n",
    "\n",
    "# TODO: set up KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# kf = KFold(...)\n",
    "\n",
    "for k in k_values:\n",
    "    Xk = X[:, :k]  # use the first k features (for simplicity)\n",
    "\n",
    "    # TODO: define Pipeline with StandardScaler and LinearRegression\n",
    "    # pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n",
    "\n",
    "    # TODO: cross_val_score with scoring=RMSE_SCORING and the KFold above\n",
    "    # scores = cross_val_score(pipe, Xk, y, scoring=RMSE_SCORING, cv=kf)\n",
    "\n",
    "    # Convert to positive RMSE\n",
    "    # rmses = -scores\n",
    "    # mean_rmses.append(rmses.mean())\n",
    "    # std_rmses.append(rmses.std())\n",
    "\n",
    "# Plot (do not set custom colors/styles)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "# TODO: uncomment after computing mean_rmses/std_rmses\n",
    "# plt.errorbar(k_values, mean_rmses, yerr=std_rmses, fmt='o-')\n",
    "plt.xlabel(\"Number of features (k)\")\n",
    "plt.ylabel(\"CV RMSE (5-fold)\")\n",
    "plt.title(\"Effect of Feature Count on CV Error\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6a810",
   "metadata": {},
   "source": [
    "\n",
    "**Reflection (write here):**  \n",
    "In 2–4 sentences, does adding more features always help here? Why might performance worsen or plateau as `k` increases?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ed2a02",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 3 — Linear Model Complexity via Polynomial Features\n",
    "\n",
    "**Goal:** Keep a **linear model** (LinearRegression) but change input representation by adding **polynomial features**. Compare degrees `{1, 2, 3}` with CV.\n",
    "\n",
    "> Even with polynomial features, `LinearRegression` is still **linear in parameters** (we’re just expanding the feature space). This isolates the effect of **feature engineering** on bias/variance.\n",
    "\n",
    "**Your task**\n",
    "1. Start with only the **first 2 original features**.  \n",
    "2. Build pipelines: `StandardScaler → PolynomialFeatures(degree=d, include_bias=False) → LinearRegression`.  \n",
    "3. For degrees `d ∈ {1, 2, 3}`, compute **5-fold CV RMSE**.  \n",
    "4. Plot degree vs mean RMSE.  \n",
    "5. Briefly discuss bias/variance trade-offs you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e7db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Polynomial Features vs. CV Error ---\n",
    "\n",
    "degrees = [1, 2, 3]\n",
    "mean_rmses_deg, std_rmses_deg = [], []\n",
    "\n",
    "# Use only first 2 features\n",
    "X2 = X[:, :2]\n",
    "\n",
    "# TODO: set up KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# kf = KFold(...)\n",
    "\n",
    "for d in degrees:\n",
    "    # TODO: define Pipeline with StandardScaler, PolynomialFeatures(degree=d, include_bias=False), LinearRegression\n",
    "    # pipe = Pipeline([\n",
    "    #     (\"scaler\", StandardScaler()),\n",
    "    #     (\"poly\", PolynomialFeatures(degree=d, include_bias=False)),\n",
    "    #     (\"lr\", LinearRegression())\n",
    "    # ])\n",
    "\n",
    "    # TODO: cross_val_score with scoring=RMSE_SCORING and the KFold above\n",
    "    # scores = cross_val_score(pipe, X2, y, scoring=RMSE_SCORING, cv=kf)\n",
    "\n",
    "    # rmses = -scores\n",
    "    # mean_rmses_deg.append(rmses.mean())\n",
    "    # std_rmses_deg.append(rmses.std())\n",
    "\n",
    "# Plot (do not set custom colors/styles)\n",
    "plt.figure()\n",
    "# TODO: uncomment after computing mean_rmses_deg/std_rmses_deg\n",
    "# plt.errorbar(degrees, mean_rmses_deg, yerr=std_rmses_deg, fmt='o-')\n",
    "plt.xlabel(\"Polynomial degree\")\n",
    "plt.ylabel(\"CV RMSE (5-fold)\")\n",
    "plt.title(\"Polynomial Features vs. CV Error (Linear Regression)\")\n",
    "plt.xticks(degrees)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601b111",
   "metadata": {},
   "source": [
    "\n",
    "**Reflection (write here):**  \n",
    "In ~3 sentences, relate your results to bias (underfitting) and variance (overfitting). When might `degree=2` help? When might `degree=3` hurt?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52298c0e",
   "metadata": {},
   "source": [
    "\n",
    "### (Optional) Extension — Fold Diagnostics\n",
    "\n",
    "Inspect fold-by-fold RMSE to see variability:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33568497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: variability check for k=10 features in Exercise 2\n",
    "\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# pipe = Pipeline([(\"scaler\", StandardScaler()), (\"lr\", LinearRegression())])\n",
    "\n",
    "# scores = cross_val_score(pipe, X[:, :10], y, scoring=RMSE_SCORING, cv=kf)\n",
    "# print(\"Fold RMSEs:\", -scores)\n",
    "# print(\"Mean ± SD:\", (-scores).mean(), \"±\", (-scores).std())\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
